{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as emotion_image.jpg\n",
      "{'label': 'sad', 'score': 0.6884231567382812}\n"
     ]
    }
   ],
   "source": [
    "# Emotion analysis\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "def capture_and_save_image():\n",
    "    # Open the default camera (usually the webcam)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open camera.\")\n",
    "        return\n",
    "    \n",
    "    # Capture a frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    # Save the captured frame\n",
    "    file_name = \"emotion_image.jpg\"\n",
    "    cv2.imwrite(file_name, frame)\n",
    "    print(f\"Image saved as {file_name}\")\n",
    "    \n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "capture_and_save_image()\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/dima806/facial_emotions_image_detection\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    return response.json()\n",
    "\n",
    "output = query(os.getcwd() + \"\\emotion_image.jpg\")\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as general_image.jpg\n",
      "[{'generated_text': 'there is a man sitting at a table with a plate of food'}]\n"
     ]
    }
   ],
   "source": [
    "# Image Summarizer\n",
    "import cv2\n",
    "import os \n",
    "\n",
    "def capture_and_save_image():\n",
    "    # Open the default camera (usually the webcam)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open camera.\")\n",
    "        return\n",
    "    \n",
    "    # Capture a frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        cap.release()\n",
    "        return\n",
    "    \n",
    "    # Save the captured frame\n",
    "    file_name = \"general_image.jpg\"\n",
    "    cv2.imwrite(file_name, frame)\n",
    "    print(f\"Image saved as {file_name}\")\n",
    "    \n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "capture_and_save_image()\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    return response.json()\n",
    "\n",
    "output = query(os.getcwd() + \"\\general_image.jpg\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something:\n",
      "You said: today weather is hot\n",
      "No image found for 'is'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052764c7a0f64dbcb6673dca2a3bf56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'RIFF\\x18\\xd1\\t\\x00WEBPVP8X\\n\\x00\\x00\\x00\\x12\\x00\\x00\\x00\\xdf\\x01\\x00\\x0c\\x01\\x00Aâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#text2ASL\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import time\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "def speech_to_text():\n",
    "    # Initialize recognizer\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    # Capture audio from the user\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something:\")\n",
    "        # Adjust for ambient noise\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        # Convert audio to text\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, I could not understand what you said.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results; {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = speech_to_text()\n",
    "    if text:\n",
    "        print(\"You said:\", text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display_word_videos(sentence, folder_path):\n",
    "    # Tokenize the input sentence into words\n",
    "    words = sentence.split()\n",
    "    \n",
    "    # Create a list to store the video widgets\n",
    "    video_widgets = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Check if a .webp file exists for the word\n",
    "        image_path = os.path.join(folder_path, f\"{word}.webp\")\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            # Create video widget for each image\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                video_data = f.read()\n",
    "            video_widget = widgets.Image(value=video_data, format='webp', width=600, height=600)\n",
    "            video_widgets.append(video_widget)\n",
    "        else:\n",
    "            print(f\"No image found for '{word}'\")\n",
    "\n",
    "    # Create a container widget to display the video widgets\n",
    "    container = widgets.VBox(video_widgets)\n",
    "    display(container)\n",
    "\n",
    "# Example usage\n",
    "#sentence = input(\"Enter a sentence: \")\n",
    "sentence = text\n",
    "folder_path = r'C:\\Users\\vidit shrama\\Downloads\\filtered_data' # Change this to the path of your image folder\n",
    "display_word_videos(sentence, folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
